{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libreria de Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('cess_esp')\n",
    "#   nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso Final POS - Ejemplo español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =  \"El oso caza abejas negras. Ademas, vive en la montaña\"\n",
    "\n",
    "#\"He estado lidiando con la instalación de los programas. No sé qué puede estar mal. El día está pesado!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline del Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libreria NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.parse import DependencyGraph\n",
    "from nltk import pos_tag\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yerzy_hqbafrm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love this pretty dog.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Segmentation:\n",
      "['I love this pretty dog.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence Segmentation:\")\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Word Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Tokenization:\n",
      "['I', 'love', 'this', 'pretty', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "print(\"\\nWord Tokenization:\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Stemming - Encontrar la raíz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemming:\n",
      "['i', 'love', 'thi', 'pretti', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"\\nStemming:\")\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lematizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatization:\n",
      "['I', 'love', 'this', 'pretty', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"\\nLemmatization:\")\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Stop Word Analysis: identificar y eliminar palabras prescindibles de un texto. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stop Word Analysis:\n",
      "['love', 'pretty', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "#Las palabras vacías son palabras de uso común que generalmente se consideran de poco valor.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "print(\"\\nStop Word Analysis:\")\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. y 7. Parsing de dependencia y speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part of Speech Tagging:\n",
      "[('love', 'VB'), ('pretty', 'RB'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = pos_tag(filtered_words)\n",
    "print(\"\\nPart of Speech Tagging:\")\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('love', 'VBP'),\n",
       " ('this', 'DT'),\n",
       " ('pretty', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(words)\n",
    "tagged[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree('S', [('I', 'PRP'), ('love', 'VBP'), ('this', 'DT'), ('pretty', 'JJ'), ('dog', 'NN'), ('.', '.'), ('Besides', 'NNP'), (',', ','), ('its', 'PRP$'), ('owner', 'NN'), ('is', 'VBZ'), ('from', 'IN'), Tree('GPE', [('Bolivia', 'NNP')]), ('!', '.'), ('His', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), Tree('PERSON', [('Sam', 'NNP')]), ('.', '.'), Tree('PERSON', [('Sam', 'NNP')]), ('loves', 'VBZ'), ('mountains', 'NNS'), ('!', '.'), ('!', '.'), ('!', '.'), ('Do', 'VBP'), ('you', 'PRP'), ('know', 'VB'), ('his', 'PRP$'), ('house', 'NN'), ('?', '.')])\n"
     ]
    }
   ],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "print(entities.__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de árboles jerárquicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharo\\Anaconda3\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'es_core_news_md' (3.3.0) was trained with spaCy v3.3.0 and may not be 100% compatible with the current version (3.6.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =  \"El oso caza abejas negras. Ademas, vive en la montaña\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El -> DET\n",
      "oso -> NOUN\n",
      "caza -> VERB\n",
      "abejas -> NOUN\n",
      "negras -> ADJ\n",
      ". -> PUNCT\n",
      "Ademas -> PROPN\n",
      ", -> PUNCT\n",
      "vive -> VERB\n",
      "en -> ADP\n",
      "la -> DET\n",
      "montaña -> NOUN\n"
     ]
    }
   ],
   "source": [
    "# create spacy \n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text,'->',token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"es\" id=\"ecfa84739dde4417bec758280ad11a3a-0\" class=\"displacy\" width=\"1800\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">El</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">oso</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">caza</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">abejas</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">negras.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Ademas,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">vive</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">en</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">la</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">montaña</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ecfa84739dde4417bec758280ad11a3a-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ecfa84739dde4417bec758280ad11a3a-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ecfa84739dde4417bec758280ad11a3a-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ecfa84739dde4417bec758280ad11a3a-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ecfa84739dde4417bec758280ad11a3a-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ecfa84739dde4417bec758280ad11a3a-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ecfa84739dde4417bec758280ad11a3a-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ecfa84739dde4417bec758280ad11a3a-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ecfa84739dde4417bec758280ad11a3a-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ecfa84739dde4417bec758280ad11a3a-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ecfa84739dde4417bec758280ad11a3a-0-5\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,89.5 1620.0,89.5 1620.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ecfa84739dde4417bec758280ad11a3a-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,266.5 L1287,254.5 1303,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ecfa84739dde4417bec758280ad11a3a-0-6\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,177.0 1615.0,177.0 1615.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ecfa84739dde4417bec758280ad11a3a-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ecfa84739dde4417bec758280ad11a3a-0-7\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,2.0 1625.0,2.0 1625.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ecfa84739dde4417bec758280ad11a3a-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1625.0,266.5 L1633.0,254.5 1617.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy \n",
    "displacy.render(doc, style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulacion de oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1 = ['Necesito un bowl de acai porfavor.',\n",
    " 'Me da un jugo de tumbo?',\n",
    " 'Vendame dos hamburguesas de barbacoa.',\n",
    " 'voy a llevar una hamburguesa mediana con papas.',\n",
    " 'A mí me gustaría la ensalada César.',\n",
    " 'para empezar, voy a pedir la sopa de tomate.',\n",
    " 'Para el postre quiero una torta de durazno',\n",
    " 'Para beber, una sangría, por favor.',\n",
    " 'De primero tomaremos la crema de verduras y una ensalada mixta.',\n",
    " 'Probaremos el arroz con leche y la tarta de chocolate.',\n",
    " 'Tomaré una cerveza, por favor.',\n",
    " '¿Me da un postre de canela?','Deme un pollo frito con curry.',\n",
    " 'Me trae una pizza de palmito',\n",
    " 'Deme tres trancapechos',\n",
    " 'Deme una chuleta de carne con huevo',\n",
    " 'Deme tres tacos con jalapenios',\n",
    " 'Traigame cinco pizzas con jamon y tomate',\n",
    " 'Quiero 6 pollos personales',\n",
    " 'Quisiera tres hamburguesas con pepino',\n",
    " 'Me trae papas fritas',\n",
    " 'vendame cinco empanadas de pollo',\n",
    " 'Llevaré tres sandwich de palta',\n",
    " 'Necesito una pasta de albahaca',\n",
    "      'Quiero llevar 2 pizzas con piña']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para ADP mark\n",
      "beber VERB ROOT\n",
      ", PUNCT punct\n",
      "una DET det\n",
      "sangría NOUN obj\n",
      ", PUNCT punct\n",
      "por ADP advmod\n",
      "favor NOUN fixed\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(txt1[7])\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_,token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuente spacy: https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/Multilingual/Spanish/03-POS-Keywords-Spanish.html\n",
    "    \n",
    "    Instalar la linea inferior solo una vez"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
